{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stein's Paradox Notebook\n",
    "\n",
    "### by Adam Kraft, adk@mit.edu.  The inspiration for this notebook comes from converations with Mike Fleder and Josh Joseph\n",
    "\n",
    "## Why\n",
    "\n",
    "Because Stein's paradox is utterly mind blowing. I don't rememer where I was when Kennedy was shot, likely because I didn't exist then. But I remember where I was when Mike told me about Stein's paradox... if you haven't heard about it, whoa, make sure you're sitting down ;-)\n",
    "\n",
    "There are lots of interesting stats and probability puzzles that blew my mind initially. The monty hall problem is an example. For reference, in the monty hall problem there are 3 doors. Behind one is a new car. behind the other two are goats. A game show contestant picks one door at random. Monty, the host, then reveals a goat behind another door (he knows where the car is, and so he knows not to reveal it to you). The contestant must now decide: switch doors or choose to open the one they chose at first. The answer is that it's always better to switch. When I first heard about it as a kid, I thought it was a lie: both choices seemed equally good. I didn't believe the math teacher---I thought, screw that, I'll write a simulation to prove that I'm right. So I did, and in the process found out *why* my intuition failed me. For the monty hall problem, you can get great intuition by messing with the numbers. Suppose there were 100 doors instead of 3, and 99 of them have goats. You pick a door, then monty reveals 98 goats behind all but one of the remaining doors. Now, it's obvious that your chances are better (or worse if you like goats) if you switch. Wheras your odds of finding a car with the door you initially picked cannot increase from 1/100, the odds if you switch are now obviously much higher (99/100) because Monty has just given up a lot of information.\n",
    "\n",
    "I tried hard to develop the same flavor of intuition for Stein's paradox and have so far failed. This notebook is for messing with the numbers until I, with some luck, develop a better intuition for Stein's insane puzzle.\n",
    "\n",
    "## What\n",
    "\n",
    "Stein's paradox takes a little bit more setup than the Monty Hall problem. Suppose we have 3 or more independent random variables. It adds emphasis to the mind-blowing aspect of this problem to think of these variables as obviously unrelated things: baseball stats, birthrates in China, Martian weather. All we know about them is that they're each drawn from some normal distribution: $X_i \\sim \\mathcal{N}(\\theta_i,1)$. For now, don't worry about the fact that the variance is 1 or even that everything's a normal distribution. With some work, a lot of distributions can be transformed to fit the preconditions of this paradox. What we want is an estimator, $\\hat{\\theta} = f(X)$, to approximate the true means $\\theta = <X_1, X_2, X_3, ... X_p>$.\n",
    "\n",
    "We of course want a good esimator, so we'll define a loss function $L(\\hat{\\theta},\\theta)$ as a way of evaluating guesses at our distribution of interest's vector of means. The loss function could be squared euclidian distance, or absolute error (taxicab distance) or some other metric. In principle, this loss could be any way of describing estimation error: it represents the penalty for error in guessing this vector of means, and so there is value to us in minimizing it. A very popular choice of loss function is mean squared error: proportional to squared euclidean distance between our estimated means and the true means. Since the estimator $\\hat{\\theta}$ depends on random observations, our loss value will be random too. To take this randomness out of the problem and be able to compare estimators to one another deterministically, we define risk, $R(\\hat{\\theta},\\theta) = E[L(\\hat{\\theta},\\theta)]$ as the expeted loss given some function for obtaining $\\hat{\\theta}$ from random obervations of $X$.\n",
    "\n",
    "In order to reason about relative risk of different estimators, statisticians define a concept of **dominance**. They say an estimator $\\hat{\\theta}$ **strictly dominates** $\\tilde{\\theta}$ if $R(\\hat{\\theta},\\theta) \\le R(\\tilde{\\theta},\\theta)$ for all $\\theta$, and the inequality is strict for some $\\theta$. For any value $\\theta$ could take, the risk, i.e. expected loss, of using estimator $\\hat{\\theta}$ is less than or equal to the risk of using estimator $\\tilde{\\theta}$. Furthermore, there is at least some $\\theta$ for which the risk of $\\hat{\\theta}$ is strictly less than $\\tilde{\\theta}$. Clearly, if we want to minimize risk, se should never use an estimator $\\tilde{\\theta}$ if we can show that it is strictly dominated by another estimator $\\hat{\\theta}$. Any estimator that we can show to be strictly dominated is called **inadmissible**. If we can show that an estimator $\\hat{\\theta}$ is not strictly dominated by any other estimator, we call $\\hat{\\theta}$ **admissible**. So far, so good.\n",
    "\n",
    "Now back to our variables: baseball, birthrates, Mars weather. Suppose we have one sample from each and we want to try to guess the means. If we consider any one variable on its own, the intuitive answer of using the sample itself as an estimate of that one variable's mean is admissible. But, **if we're concerned with minimizing the overall risk of all three variables combined, this intuitive estimator is inadmissible!**. That's right. In \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
